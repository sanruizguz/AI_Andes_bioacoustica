{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tunning BirdNET (AKA shallow classifier)\n",
    "\n",
    "Para esta sesion vamos a cargar el ambiente virtual que creamos para la primera practica ademas del modelo \"Zoo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from glob import glob\n",
    "import sklearn\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from pathlib import Path\n",
    "\n",
    "#set up plotting\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[15,5] #for large visuals\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# opensoundscape transfer learning tools\n",
    "from opensoundscape.ml.shallow_classifier import MLPClassifier, quick_fit, fit_classifier_on_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define las \"Seeds\"\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparaci√≥n  del set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego de mover la carpeta descargada, define la ruta:\n",
    "dataset_path = Path(\"./rana_sierrae_2022/\")\n",
    "\n",
    "# Dado que usaremos el modelo \"BirdNET\", es necesario estandarizar la duracion de todos los clips a 3 segundos\n",
    "from opensoundscape.annotations import BoxedAnnotations\n",
    "\n",
    "audio_and_raven_files = pd.read_csv(f\"{dataset_path}/audio_and_raven_files.csv\")\n",
    "\n",
    "# Actualiza la lista donde estan guardados los audios y anotaciones\n",
    "audio_and_raven_files[\"audio\"] = audio_and_raven_files[\"audio\"].apply(\n",
    "    lambda x: f\"{dataset_path}/{x}\"\n",
    ")\n",
    "audio_and_raven_files[\"raven\"] = audio_and_raven_files[\"raven\"].apply(\n",
    "    lambda x: f\"{dataset_path}/{x}\"\n",
    ")\n",
    "\n",
    "# Crea las tabla con anotaciones (similar a la primera sesion practica)\n",
    "annotations = BoxedAnnotations.from_raven_files(\n",
    "    raven_files=audio_and_raven_files[\"raven\"],\n",
    "    audio_files=audio_and_raven_files[\"audio\"],\n",
    "    annotation_column=\"annotation\",\n",
    ")\n",
    "\n",
    "# Ahora generemos etiquetas para clips cada 3 segundos, incluyendo cualquier etiqueta que se sobrelape por al menos 0.2s\n",
    "labels = annotations.clip_labels(clip_duration=3, min_label_overlap=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora inspeccionemos las etiquetas\n",
    "labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividamos las etiquetas para crear sets de entrenamiento y evaluacion (No recomendado)\n",
    "labels_train, labels_val = sklearn.model_selection.train_test_split(labels[[\"A\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioacoustics_model_zoo as bmz\n",
    "\n",
    "# Primero, carguemos BirdNET de Zoo\n",
    "birdnet = bmz.BirdNET()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora generemos embeddings a para cada uno de los elementos en nuestros datasets\n",
    "emb_train = birdnet.embed(labels_train, return_dfs=False, batch_size=128, num_workers=0)\n",
    "emb_val = birdnet.embed(labels_val, return_dfs=False, batch_size=128, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para entrenar el clasificador sobre la clase \"A\" debemos reemplazar el output de la fully-connected layer con una capa de 1-output para clase \"A\" \n",
    "classes = [\"A\"]\n",
    "birdnet.change_classes(classes)\n",
    "\n",
    "# fit the classification head with embeddings and labels\n",
    "birdnet.network.fit(emb_train, labels_train.values, emb_val, labels_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos predicciones pasando los embeddings a traves del clasificador\n",
    "preds = birdnet.network(torch.tensor(emb_val)).detach()\n",
    "# Calculemos el AUC-ROC\n",
    "roc_auc_score(labels_val.values, preds, average=None)\n",
    "\n",
    "# Veamos los histogramas con los scores\n",
    "preds = preds.detach().numpy()\n",
    "plt.hist(preds[labels_val == True], bins=20, alpha=0.5, label=\"positives\")\n",
    "plt.hist(preds[labels_val == False], bins=20, alpha=0.5, label=\"negatives\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensoundscape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
